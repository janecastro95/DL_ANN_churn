# -*- coding: utf-8 -*-
"""ANN practical implementation (classification).ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1P0Q8IwNjswT4jAkJJcIDEFG4Fae7E61H

# ANN practical implementation

### Configurações
"""

!pip install tensorflow

"""###"""

import tensorflow as tf
print(tf.__version__)



import numpy as np
import matplotlib.pyplot as plt
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import confusion_matrix, accuracy_score

from tensorflow.keras.models import Sequential #para forward and back propagation
from tensorflow.keras.layers import Dense #para criar as camadas (input, hidden, output layers)
from tensorflow.keras.layers import LeakyReLU,PReLU, ELU, ReLU #funções de ativação
from tensorflow.keras.layers import Dropout #percentual de neuronios que devem ser desativados em cada camada, para evitar overfitting

"""### Import and prepare dataset"""

dataset = pd.read_csv('Churn_Modelling.csv')

dataset.head()

## Divide into dependent and independent features

X=dataset.iloc[:,3:-1]
Y=dataset.iloc[:,-1]

# Feature Engineering

geography_ohe = pd.get_dummies(X['Geography'],drop_first=True).astype(int)
gender_ohe = pd.get_dummies(X['Gender'],drop_first=True).astype(int)

#X = X.merge(geography_ohe,left_index=True,right_index=True)
#X = X.merge(gender_ohe,left_index=True,right_index=True)
#X = X.drop(['Geography','Gender'],axis=1)
#X.head()

X = pd.concat([X, geography_ohe, gender_ohe],axis=1)
X = X.drop(columns=['Geography','Gender'])

#Split dataset into training and test dataset
X_train, X_test, Y_train, Y_test = train_test_split(X,Y,test_size=0.2, random_state=22)

sc = StandardScaler()
X_train = sc.fit_transform(X_train)
X_test = sc.transform(X_test)

"""### ANN"""

#Initialize the ANN
classifier=Sequential()

#Add the input layer
classifier.add(Dense(units=11, activation='relu')) #units = número de variáveis | activation é sempre da próxima camada

#Add the first hidden layer
classifier.add(Dense(units=7, activation='relu'))

#Add the second hidden layer
classifier.add(Dense(units=6, activation='relu'))
classifier.add(Dropout(0.3)) #para essa camada, remove 0.3 dos neurônios

#Add the output layer
classifier.add(Dense(units=1, activation='sigmoid')) #Sigmoid porque a saída é binária

#Define the optimizer e loss function

#classifier.compile(optimizer='adam',loss='binary_crossentropy',metrics=['accuracy'])
#adam default usa learning_rate = 0.5, mas é possível escolher:

opt=tf.keras.optimizers.Adam(learning_rate=0.01)
classifier.compile(optimizer=opt,loss='binary_crossentropy',metrics=['accuracy'])

#Define a Early stopping

early_stopping = tf.keras.callbacks.EarlyStopping(
                                      monitor="val_loss",
                                      min_delta=0.0001,
                                      patience=20,
                                      verbose=1,
                                      mode="auto",
                                      baseline=None,
                                      restore_best_weights=False,
                                      start_from_epoch=0,
                                  ) #Stop training when a monitored metric has stopped improving

model=classifier.fit(X_train,
                    Y_train,
                    validation_split=0.33,
                    batch_size=10,
                    epochs=1000,
                    callbacks=early_stopping)

#Ao longo das iteações é possível ver a acurácia aumentando e a perda diminuindo

model.history.keys()

plt.plot(model.history['accuracy'])
plt.plot(model.history['val_accuracy'])
plt.legend(['train','val'])
plt.title('ACCURACY')
plt.show()

plt.plot(model.history['loss'])
plt.plot(model.history['val_loss'])
plt.legend(['train','val'])
plt.title('LOSS')
plt.show()

#Weights: pode salvar como pickle para poder usar posteriormente
classifier.get_weights()

"""## Evaluation"""

dataset.groupby('Exited')['CustomerId'].count()/dataset.shape[0]

y_pred = classifier.predict(X_test)
y_pred = (y_pred >= 0.5) #1 se for maior que 0,5; 0 c.c.

y_pred_train = classifier.predict(X_train)
y_pred_train = (y_pred_train >= 0.5) #1 se for maior que 0,5; 0 c.c.

acc_test = accuracy_score(y_pred,Y_test)
acc_test

acc_train = accuracy_score(y_pred_train,Y_train)
acc_train